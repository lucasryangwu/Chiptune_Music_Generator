import collections

import numpy as np
import pandas as pd
import pygame
import time
import os
from music21 import *
import mido
import pretty_midi
import matplotlib.pyplot as plt

from typing import List, Tuple, Union, Optional
import math, glob, random
from typing import List, Tuple

from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from model_classes import ConvAutoencoder, SequencePredictor


#----------------------------------------------------------------------------------------------------
###        --- Data Exploration ---
#----------------------------------------------------------------------------------------------------

def play_midi(file_path, volume = 1.0):
    """
    Play a MIDI file using Pygame

    Args:
        file_path (str): Path to the MIDI file (.mid)
        volume (float): Playback volume (0.0 to 1.0)
    """
    # Initialize pygame mixer
    pygame.mixer.init()
    
    # Load the MIDI file
    pygame.mixer.music.load(file_path)
    
    # Set volume
    pygame.mixer.music.set_volume(volume)
    
    # Print start message
    print(f"Playing: {os.path.basename(file_path)}")
    
    try:
        # Play the MIDI file
        pygame.mixer.music.play()
        
        # Wait while the music is playing
        while pygame.mixer.music.get_busy():
            time.sleep(0.1)
        
        # Print stop message
        print(f"Finished playing: {os.path.basename(file_path)}")
    
    except KeyboardInterrupt:
        # Stop the music if interrupted
        pygame.mixer.music.stop()
        print(f"\nPlayback interrupted: {os.path.basename(file_path)}")
    
    finally:
        # Ensure pygame mixer is properly cleaned up
        pygame.mixer.music.unload()

#----------------------------------------------------------------------------------------------------

def midi_to_notes(midi_file: str, tracks=None) -> pd.DataFrame:
    """
    Extract note data from a MIDI file.

    Parameters
    ----------
    midi_file : str
        Path to MIDI file.
    tracks : None, int, or list of ints
        - None → extract from ALL instrument tracks
        - int → extract ONLY that track index
        - list[int] → extract from those track indices

    Returns
    -------
    pd.DataFrame with columns:
        ['instrument', 'pitch', 'start', 'end', 'step', 'duration']
    """

    pm = pretty_midi.PrettyMIDI(midi_file)
    all_instruments = pm.instruments

    # Normalize track selection input
    if tracks is None:
        track_indices = range(len(all_instruments))
    elif isinstance(tracks, int):
        track_indices = [tracks]
    else:
        track_indices = tracks

    all_notes = collections.defaultdict(list)

    # Process each requested track
    for idx in track_indices:
        instrument = all_instruments[idx]
        instrument_name = pretty_midi.program_to_instrument_name(instrument.program)

        # Skip empty tracks
        if len(instrument.notes) == 0:
            continue

        sorted_notes = sorted(instrument.notes, key=lambda n: n.start)
        prev_start = sorted_notes[0].start

        for note in sorted_notes:
            start = note.start
            end = note.end
            note_name = pretty_midi.note_number_to_name(note.pitch)

            all_notes["track"].append(idx)
            all_notes["instrument"].append(instrument_name)
            all_notes["pitch"].append(note.pitch)
            all_notes["note_name"].append(note_name)
            all_notes["start"].append(start)
            all_notes["end"].append(end)
            all_notes["duration"].append(end - start)

            prev_start = start

    # Convert everything to a DataFrame
    return pd.DataFrame({k: np.array(v) for k, v in all_notes.items()})

# sample usage
#raw_notes = midi_to_notes(sample_file)
#raw_notes.head()

# new dataframe with only track 0
# track_1_notes = raw_notes[note_info['track']==0].drop(columns=['track', 'instrument'])
# print(track_1_notes.shape)
# track_1_notes.head()

#----------------------------------------------------------------------------------------------------

def notes_to_midi(notes: pd.DataFrame, out_file: str, velocity: int = 100):
    """
    Reconstruct a multi-track MIDI file from a dataframe containing:
        'track', 'instrument', 'pitch', 'step', 'duration'
    (plus optional note_name, start, end, etc. which are ignored)

    Parameters
    ----------
    notes : pd.DataFrame
        MIDI notes dataframe generated by your extraction pipeline.
    out_file : str
        Path to write the resulting MIDI file.
    velocity : int
        MIDI note velocity for all notes.

    Returns
    -------
    pretty_midi.PrettyMIDI
    """

    pm = pretty_midi.PrettyMIDI()

    # Ensure track column
    if "track" not in notes.columns:
        notes = notes.copy()
        notes["track"] = 0

    # Group by track
    for track_id, track_df in notes.groupby("track"):
        inst_name = track_df["instrument"].iloc[0]
        program = pretty_midi.instrument_name_to_program(inst_name)

        instrument = pretty_midi.Instrument(program=program)

        # Sort by actual start time
        track_df = track_df.sort_values("start")

        for _, row in track_df.iterrows():
            note = pretty_midi.Note(
                velocity=velocity,
                pitch=int(row["pitch"]),
                start=float(row["start"]),
                end=float(row["end"]),
            )
            instrument.notes.append(note)

        pm.instruments.append(instrument)

    pm.write(out_file)
    return pm

# sample usage
#midi_to_notes(raw_notes, 'sample_output.mid')
#play_midi('sample_output.mid')

#----------------------------------------------------------------------------------------------------

def plot_piano_roll(df, title="Piano Roll"):
    """
    Plots a multi-track piano roll from the processed MIDI dataframe.

    Expected columns:
        'pitch', 'start', 'end'
    Optional:
        'track', 'instrument'

    Parameters
    ----------
    df : pd.DataFrame
        Output from your midi-to-notes processing function.
    title : str
        Title for the full figure.
    """

    # Determine if we have a track column
    if "track" in df.columns:
        tracks = sorted(df["track"].unique())
    else:
        tracks = [0]   # Treat as single track
        df = df.copy()
        df["track"] = 0

    num_tracks = len(tracks)

    fig, axes = plt.subplots(
        num_tracks, 1,
        figsize=(14, 3 * num_tracks),
        sharex=True,
        constrained_layout=True
    )

    # If only one axis, make it a list for consistency
    if num_tracks == 1:
        axes = [axes]

    for i, track_id in enumerate(tracks):
        ax = axes[i]
        track_df = df[df["track"] == track_id]

        # Piano roll rectangles
        for _, row in track_df.iterrows():
            ax.hlines(
                y=row["pitch"],
                xmin=row["start"],
                xmax=row["end"],
                linewidth=4,
            )

        # Labeling
        instrument_name = ""
        if "instrument" in track_df.columns:
            instrument_name = track_df["instrument"].iloc[0]

        ax.set_ylabel("Pitch (MIDI)")
        ax.set_title(f"Track {track_id}: {instrument_name}")
        ax.grid(True, linestyle="--", alpha=0.3)

    axes[-1].set_xlabel("Time (seconds)")
    fig.suptitle(title, fontsize=16, y=1.08)

    plt.show()

#----------------------------------------------------------------------------------------------------
###        --- Data Preprocessing ---
#----------------------------------------------------------------------------------------------------

def normalize_note_velocities(
    path_in, path_out,
    min_vel=40, max_vel=100,
    default_volume=100,       # CC7
    default_expression=100    # CC11
):
    mid = mido.MidiFile(path_in, clip=True)

    new_tracks = []
    track_channel = 0

    for track in mid.tracks:
        new_track = mido.MidiTrack()

        for msg in track:
            # 1. Replace "note_on with velocity=0" → note_off
            if msg.type == 'note_on' and msg.velocity == 0:
                msg = mido.Message(
                    'note_off',
                    note=msg.note,
                    velocity=64,
                    time=msg.time,
                    channel=track_channel % 16
                )

            # 2. Normalize velocities for real note_on events
            elif msg.type == 'note_on':
                velocity = max(min(msg.velocity, max_vel), min_vel)
                msg = mido.Message(
                    'note_on',
                    note=msg.note,
                    velocity=velocity,
                    time=msg.time,
                    channel=track_channel % 16
                )

            # 3. Ensure note_off messages get correct channel
            elif msg.type == 'note_off':
                msg = mido.Message(
                    'note_off',
                    note=msg.note,
                    velocity=msg.velocity,
                    time=msg.time,
                    channel=track_channel % 16
                )

            # 4. Fix CC7/CC11 volume/expression
            elif msg.type == 'control_change':
                value = msg.value
                if msg.control == 7:   # Volume
                    value = max(value, default_volume)
                if msg.control == 11:  # Expression
                    value = max(value, default_expression)

                msg = mido.Message(
                    'control_change',
                    control=msg.control,
                    value=value,
                    time=msg.time,
                    channel=track_channel % 16
                )

            # 5. Other messages → preserve but ensure channel if relevant
            elif hasattr(msg, "channel"):
                msg = msg.copy(channel=track_channel % 16)
            else:
                msg = msg.copy()

            new_track.append(msg)

        new_tracks.append(new_track)
        track_channel += 1

    # Replace tracks with cleaned ones
    mid.tracks = new_tracks
    mid.save(path_out)

#----------------------------------------------------------------------------------------------------
###        --- Model Training ---
#----------------------------------------------------------------------------------------------------

def build_tonnetz_mapping(rows: int = 24, cols: int = 12, base_pitch: int = 60):
    """
    Returns info for mapping MIDI pitch -> (row, col) on a rows x cols tonnetz grid.
    base_pitch is a reference pitch (e.g., C4=60) used as anchor.
    We'll place the 'center' row at rows//2 which corresponds roughly to base_pitch offsets.
    """
    row_center = rows // 2
    # Precompute pitch value per grid cell via formula p = base + c*7 + (r-row_center)*4
    grid_pitch = np.zeros((rows, cols), dtype=int)
    for r in range(rows):
        for c in range(cols):
            grid_pitch[r, c] = base_pitch + c * 7 + (r - row_center) * 4
    return grid_pitch, row_center

#----------------------------------------------------------------------------------------------------

def midi_pitch_to_tonnetz_coords(pitch: int, grid_pitch: np.ndarray) -> Tuple[int,int]:
    """
    Map a MIDI pitch integer to the nearest (r,c) in grid_pitch by absolute semitone difference.
    """
    # grid_pitch shape (rows, cols)
    diffs = np.abs(grid_pitch - pitch)
    idx = np.unravel_index(np.argmin(diffs), diffs.shape)
    return int(idx[0]), int(idx[1])

#----------------------------------------------------------------------------------------------------

def midi_to_tonnetz_sequence(midi_path: str,
                             rows: int = 24,
                             cols: int = 12,
                             quantize_beat: float = 1.0,
                             base_pitch: int = 60,
                             transpose_to_c: bool = False) -> np.ndarray:
    """
    Convert a MIDI file to a sequence of tonnetz binary matrices.
    Returns array shape: (T, rows, cols) where T = number of time slices (beats).
    quantize_beat: size of time slice in beats (1.0 follows the paper).
    transpose_to_c: if True, attempt to transpose piece to C major/A minor (optional).
    """
    pm = pretty_midi.PrettyMIDI(midi_path)
    # optionally transpose: naive heuristic using key signature from pretty_midi (if available)
    if transpose_to_c:
        try:
            key_num = pm.key_signature_changes[0].key_number
            # key_number: 0=C, etc. pretty_midi may use -1 when unknown; this is optional.
        except Exception:
            key_num = None
        # skipping robust transposition for simplicity — user can preprocess externally

    # derive tempo/beats using pretty_midi
    beat_times = pm.get_beats()  # times of beats
    if len(beat_times) < 2:
        # fallback: create uniform beat grid using total_time / quantization
        total_t = pm.get_end_time()
        n_slices = max(1, int(math.ceil(total_t / (pm.get_tempo_changes()[1][0] if pm.get_tempo_changes()[0].size else 0.5))))
        beat_times = np.linspace(0, pm.get_end_time(), n_slices+1)

    # build grid mapping
    grid_pitch, _ = build_tonnetz_mapping(rows=rows, cols=cols, base_pitch=base_pitch)

    # Build event list quantized to beat grid indices
    # We'll sample time slices of length 1 beat by default. Use beat_times as grid boundaries.
    # If beat_times is irregular, fallback to fixed quantization:
    # Convert absolute beat times to boundaries [0, dt, 2dt, ...] using tempo median
    dur = pm.get_end_time()
    # We'll quantize using 1.0 beat length in seconds approximated by median beat interval
    if len(beat_times) >= 2:
        dt = float(np.median(np.diff(beat_times)))
    else:
        dt = 0.5
    n_slices = int(math.ceil(dur / dt)) + 1

    # initialize empty sequence
    seq = np.zeros((n_slices, rows, cols), dtype=np.uint8)

    # Iterate notes and fill slices
    for inst in pm.instruments:
        for n in inst.notes:
            # mark all slices overlapping [start, end)
            start_idx = int(math.floor(n.start / dt))
            end_idx = int(math.ceil(n.end / dt))
            for t in range(start_idx, min(n_slices, end_idx)):
                r, c = midi_pitch_to_tonnetz_coords(n.pitch, grid_pitch)
                seq[t, r, c] = 1

    # trim trailing empty slices
    nonempty = np.any(seq.reshape(n_slices, -1), axis=1)
    if np.any(nonempty):
        last = np.max(np.where(nonempty)[0])
        seq = seq[:last+1]
    else:
        seq = seq[:1]

    return seq.astype(np.float32)  # (T, rows, cols)

#----------------------------------------------------------------------------------------------------

def pretrain_autoencoder(autoenc: ConvAutoencoder, dataloader, device,
                         criterion = "logit", epochs=10, lr=1e-3,
                         verbose=False, debug=False):
    autoenc.to(device)
    optim = torch.optim.Adam(autoenc.parameters(), lr=lr)
    if criterion == "logit":
        criterion = nn.BCEWithLogitsLoss()
    elif criterion == "mse":
        criterion = nn.MSELoss()
    elif criterion == "l1":
        criterion = nn.L1Loss()
    autoenc.train()

    for epoch in range(epochs):
        total_loss = 0.0
        num_batches = len(dataloader)

        # Determine the interval: every 1% of total batches
        if verbose and num_batches > 0:
            interval = max(1, num_batches // 100)
            print(f"\n=== Autoencoder Pretraining Epoch {epoch+1}/{epochs} ===")
            print(f"Total batches = {num_batches} | Printing every {interval} batches (~1%)")

        for batch_idx, (X, _) in enumerate(dataloader):
            B, seq_len, ch, H, W = X.shape
            images = X.view(B * seq_len, ch, H, W).to(device)

            if debug:
                with torch.no_grad():
                    recon, _ = autoenc(images)
                    print("RECON min/max:", recon.min().item(), recon.max().item())


            optim.zero_grad()
            recon_logits, _ = autoenc(images)
            loss = criterion(recon_logits, images)
            loss.backward()
            optim.step()

            batch_loss = loss.item() * images.size(0)
            total_loss += batch_loss

            # Print progress every 1% of total batches
            if verbose and (batch_idx % interval == 0):
                pct = int((batch_idx / num_batches) * 100)
                print(f"  Training: {pct}% Complete. "
                      f"(Batch {batch_idx+1}/{num_batches}: loss={loss.item():.6f})")

        # Compute average loss
        avg = (total_loss /
               (len(dataloader.dataset) * dataloader.batch_size)
               if len(dataloader) > 0 else total_loss)

        print(f"[AE] Epoch {epoch+1}/{epochs} avg_loss={avg:.6f}")

    return autoenc

#----------------------------------------------------------------------------------------------------

def train_sequence_model(
        autoenc: ConvAutoencoder,
        seq_model: SequencePredictor,
        train_loader,
        val_loader,
        device,
        epochs=30,
        lr=1e-3,
        freeze_encoder=True,
        verbose=False,
        patience=5,
        min_delta=1e-4):

    history_train = []
    history_val   = []

    # freeze encoder if requested
    if freeze_encoder:
        for p in autoenc.parameters():
            p.requires_grad = False

    autoenc.to(device)
    seq_model.to(device)

    optim = torch.optim.Adam(
        filter(lambda p: p.requires_grad, seq_model.parameters()),
        lr=lr
    )
    criterion = nn.BCEWithLogitsLoss()

    best_val_loss = float('inf')
    best_state = None
    epochs_without_improvement = 0

    # ---------------------------
    # MAIN TRAINING LOOP
    # ---------------------------
    for epoch in range(epochs):
        seq_model.train()
        total_loss = 0.0
        num_batches = len(train_loader)

        # progress interval (~1%)
        interval = max(1, num_batches // 100) if verbose else None

        if verbose:
            print(f"\n=== Sequence Model Epoch {epoch+1}/{epochs} ===")
            print(f"Total training batches = {num_batches}")

        # ---------------------------
        # TRAINING EPOCH
        # ---------------------------
        for batch_idx, (X, y) in enumerate(train_loader):
            B, seq_len, ch, H, W = X.shape

            # flatten time frames for encoder
            X_flat = X.view(B * seq_len, ch, H, W).to(device)

            # extract latents
            with torch.no_grad():
                _, z_flat = autoenc(X_flat)

            # restore time structure: (B, seq_len, latent_dim)
            z_seq = z_flat.view(B, seq_len, -1).to(device)
            y = y.to(device)

            optim.zero_grad()
            logits = seq_model(z_seq)        # output: (B, 1, H, W)
            loss = criterion(logits, y)
            loss.backward()
            optim.step()

            total_loss += loss.item() * B

            # 1% progress print
            if verbose and (batch_idx % interval == 0):
                pct = int((batch_idx / num_batches) * 100)
                print(f"  Training: {pct}% Complete. "
                      f"(Batch {batch_idx+1}/{num_batches}: loss={loss.item():.6f})")

        train_avg = total_loss / len(train_loader.dataset)

        # ---------------------------
        # VALIDATION EPOCH
        # ---------------------------
        seq_model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for X, y in val_loader:
                B, seq_len, ch, H, W = X.shape
                X_flat = X.view(B * seq_len, ch, H, W).to(device)

                # encode
                _, z_flat = autoenc(X_flat)
                z_seq = z_flat.view(B, seq_len, -1).to(device)
                y = y.to(device)

                logits = seq_model(z_seq)
                loss = criterion(logits, y)
                val_loss += loss.item() * B

        val_avg = val_loss / len(val_loader.dataset)

        history_train.append(train_avg)
        history_val.append(val_avg)

        print(f"[SEQ] Epoch {epoch+1}/{epochs} train={train_avg:.6f} val={val_avg:.6f}")

        # ---------------------------
        # EARLY STOPPING CHECK
        # ---------------------------
        if val_avg + min_delta < best_val_loss:
            best_val_loss = val_avg
            best_state = seq_model.state_dict()
            epochs_without_improvement = 0
            if verbose:
                print(f"  ✓ New best validation loss: {best_val_loss:.6f}")
        else:
            epochs_without_improvement += 1
            if verbose:
                print(f"  ✗ No improvement ({epochs_without_improvement}/{patience})")

            if epochs_without_improvement >= patience:
                print("Early stopping triggered — restoring best sequence model.")
                seq_model.load_state_dict(best_state)
                return seq_model

    # restore best model at end
    if best_state is not None:
        seq_model.load_state_dict(best_state)

    return seq_model, history_train, history_val

#----------------------------------------------------------------------------------------------------

def plot_training_history(train_hist, val_hist, title="Training History"):
    epochs = np.arange(1, len(train_hist) + 1)

    plt.figure(figsize=(10, 5))
    plt.plot(epochs, train_hist, label="Training Loss", color="green", marker="o")
    plt.plot(epochs, val_hist, label="Validation Loss", color="deeppink", marker="o")

    plt.xticks(np.arange(0, len(train_hist) + 1, 1))

    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.grid(True, linestyle="--", alpha=0.5)
    plt.legend()
    plt.tight_layout()
    plt.show()

#----------------------------------------------------------------------------------------------------
###        --- Model Usage: Music Generation ---
#----------------------------------------------------------------------------------------------------

@torch.no_grad()
def generate_sequence(seed_seq: Union[np.ndarray, torch.Tensor], autoenc: ConvAutoencoder, seq_model: SequencePredictor,
                      gen_steps: int = 32, device: str = 'cpu', sample_prob: bool = False,
                      num_chains: int = 1, batch_size: Optional[int] = None) -> np.ndarray:
    """
    Optimized vectorized generation.
    - seed_seq: numpy array or tensor, shape (B, seed_len, H, W) for batch or (seed_len, H, W) for single.
    - gen_steps: Steps to generate per chain.
    - sample_prob: If True, sample from probabilities (Bernoulli) instead of thresholding.
    - num_chains: Number of times to chain generations (for longer sequences).
    - batch_size: If provided and seed_seq is single, replicate it to this batch size.
    Returns generated sequences as numpy array: (B, seed_len + gen_steps * num_chains, H, W)
    """
    autoenc.eval()
    seq_model.eval()
    autoenc.to(device)
    seq_model.to(device)

    # Handle input: Convert to tensor, add batch dim if needed, handle replication
    if isinstance(seed_seq, np.ndarray):
        seed_seq = torch.tensor(seed_seq).float()
    if seed_seq.ndim == 3:  # Single sequence: (seed_len, H, W) -> (1, seed_len, H, W)
        seed_seq = seed_seq.unsqueeze(0)
    B, seed_len, H, W = seed_seq.shape

    if batch_size is not None and batch_size > B:
        # Replicate the single seed to batch_size
        seed_seq = seed_seq.repeat(batch_size, 1, 1, 1)
        B = batch_size

    # Initialize generated list with seeds (on device)
    generated = seed_seq.clone().to(device)  # (B, seed_len, H, W)

    # Pre-encode the initial seed to latent space
    seed_t = generated.unsqueeze(2)  # (B, seed_len, 1, H, W) for encoder
    z_seq = []
    for i in range(seed_len):
        _, z = autoenc(seed_t[:, i])
        z_seq.append(z)
    z_seq = torch.stack(z_seq, dim=1)  # (B, seed_len, latent_dim)

    for chain in range(num_chains):
        current_generated = []  # For this chain's new frames
        for step in range(gen_steps):
            # Predict next logits
            logits = seq_model(z_seq)  # (B, 1, H, W)
            probs = torch.sigmoid(logits).squeeze(1)  # (B, H, W)

            # Binarize or sample
            if sample_prob:
                next_frame = torch.bernoulli(probs).float()  # (B, H, W)
            else:
                next_frame = (probs > 0.5).float()  # (B, H, W)

            current_generated.append(next_frame)

            # Encode next frame to latent and shift z_seq
            nf_t = next_frame.unsqueeze(1)  # (B, 1, H, W)
            _, z_nf = autoenc(nf_t)  # (B, latent_dim)
            z_seq = torch.cat([z_seq[:, 1:], z_nf.unsqueeze(1)], dim=1)  # Shift: (B, seed_len, latent_dim)

        # Append new frames to generated
        new_frames = torch.stack(current_generated, dim=1)  # (B, gen_steps, H, W)
        generated = torch.cat([generated, new_frames], dim=1)  # (B, total_len, H, W)

        # For next chain: Update z_seq to the last seed_len frames' latents (if chaining)
        if chain < num_chains - 1:
            last_seed_t = generated[:, -seed_len:].unsqueeze(2)  # (B, seed_len, 1, H, W)
            z_seq = []
            for i in range(seed_len):
                _, z = autoenc(last_seed_t[:, i])
                z_seq.append(z)
            z_seq = torch.stack(z_seq, dim=1)  # (B, seed_len, latent_dim)

    return generated.cpu().numpy()  # (B, total_len, H, W)

#----------------------------------------------------------------------------------------------------

def tonnetz_sequence_to_midi(seq: np.ndarray,
                                   base_pitch: int = 60,
                                   dt: float = 0.5,
                                   programs: list = None) -> pretty_midi.PrettyMIDI:
    """
    Convert a multi-instrument Tonnetz sequence back into a PrettyMIDI object.

    seq: numpy array shaped (T, N_instruments, rows, cols)
    base_pitch: same base pitch used in encoder
    dt: duration of each time slice in seconds (must match encoder)
    programs: list of MIDI program numbers, length N_instruments.
              If None, all instruments default to Acoustic Grand Piano (0).

    Returns:
        pretty_midi.PrettyMIDI instance
    """

    # If seq is single-instrument (T, rows, cols), expand to (T,1,rows,cols)
    if seq.ndim == 3:
        seq = seq[:, None, :, :]

    T, N_instruments, rows, cols = seq.shape

    # Build pitch-mapping grid
    grid_pitch, row_center = build_tonnetz_mapping(rows=rows, cols=cols, base_pitch=base_pitch)

    # MIDI container
    pm = pretty_midi.PrettyMIDI()

    # Default programs if not given
    if programs is None:
        programs = [0] * N_instruments

    # Create instrument objects
    instruments = [
        pretty_midi.Instrument(program=programs[i]) for i in range(N_instruments)
    ]

    # Track ongoing notes per instrument: dict[(inst, r, c)] = start_time
    ongoing = [{} for _ in range(N_instruments)]

    # --- MAIN DECODE LOOP ---
    for t in range(T):
        current_time = t * dt
        for i in range(N_instruments):
            inst_seq = seq[t, i]  # (rows, cols)
            for r in range(rows):
                for c in range(cols):
                    active = inst_seq[r, c] > 0.5
                    key = (r, c)

                    if active:
                        # start note if it's new
                        if key not in ongoing[i]:
                            ongoing[i][key] = current_time
                    else:
                        # end note if it was active
                        if key in ongoing[i]:
                            start_time = ongoing[i].pop(key)
                            pitch = int(grid_pitch[r, c])

                            note = pretty_midi.Note(
                                velocity=100,
                                pitch=pitch,
                                start=start_time,
                                end=current_time
                            )
                            instruments[i].notes.append(note)

    # Close any remaining notes at end of sequence
    end_time = T * dt
    for i in range(N_instruments):
        for (r, c), start_time in ongoing[i].items():
            pitch = int(grid_pitch[r, c])
            note = pretty_midi.Note(
                velocity=100,
                pitch=pitch,
                start=start_time,
                end=end_time
            )
            instruments[i].notes.append(note)

    # Add all instruments to the MIDI file
    for inst in instruments:
        pm.instruments.append(inst)

    return pm

#----------------------------------------------------------------------------------------------------

def chiptune_generator(input_dir, base_song, seq_len=16):
    """
    Generates new instrument tracks based on an inputted chiptune song.

    :param base_song: a MIDI file from the test dataset folder (ending in .mid)
    :param seq_len: Length of base song used in generating
    :return:
    """
    seed_path = f"{input_dir}{base_song}"

    # Converting seed to Tonnetz sequence
    try:
        seq = midi_to_tonnetz_sequence(
                        seed_path,
                        rows=24,
                        cols=12,
                        quantize_beat=1.0
                    )
        print(f"✓ Loaded {base_song}. \nShape = {seq.shape}")
    except Exception as e:
        print(f"✗ ERROR processing file:\033[0m")
        print(f"\033[0m", e)

    # Generating music from seed
    if len(seq) > 0:
        seed = seq[:seq_len]
        print("Seed shape:", seed.shape)

        generated = generate_sequence(
            seed, autoenc, seq_model, gen_steps=64, device=device,
            sample_prob=True, num_chains=2, batch_size=4
        )
        print("Generated sequences shape:", generated.shape)

    # Reverting to MIDI format and saving file
    output_dir = f"{PROJECT_PATH}/music/generated/{base_song[:-4]}"
    os.makedirs(output_dir, exist_ok=True)  # Create if it doesn't exist

    #-- Saving generated MIDI(s) in the new subfolder
    if generated.ndim == 4 and generated.shape[0] > 1:  # Batched case
        for i in range(generated.shape[0]):
            pm = tonnetz_sequence_to_midi(generated[i])
            pm.write(f"{output_dir}/generated_{i}.mid")
    else:  # Single sequence (flatten batch dim if needed)
        single_generated = generated[0] if generated.ndim == 4 else generated
        test_pm = tonnetz_sequence_to_midi(single_generated)
        test_pm.write(f"{output_dir}/generated.mid")

    print(f"Generated MIDI(s) saved to: {output_dir}")

    # Plotting notes
    generated_notes = midi_to_notes(f"{output_dir}/generated_0.mid")
    plot_piano_roll(generated_notes, title="Piano Roll - Generated MIDI")

    # Playing generated track
    play_midi(f"{output_dir}/generated_0.mid", volume = 1.0)

#----------------------------------------------------------------------------------------------------