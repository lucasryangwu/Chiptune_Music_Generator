{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM Model Training",
   "id": "c76912b91d700c57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "d4d699c7db72b3d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pygame')\n",
    "\n",
    "from funcs import *\n",
    "from model_classes import *"
   ],
   "id": "c0f41cec685468af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining Constants",
   "id": "4d4ea74d49e623d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# File Paths\n",
    "PROJECT_PATH = PROJECT_PATH = Path.cwd()  # Assumes notebook is run from project root\n",
    "DATA_PATH = f\"{PROJECT_PATH}music/cleaned_data/\"    # must have 'train', 'test', and 'valid' subfolders\n",
    "\n",
    "# Other variables\n",
    "CIRCLE_OF_FIFTHS = [0, 7, 2, 9, 4, 11, 6, 1, 8, 3, 10, 5]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # (testing with cpu)"
   ],
   "id": "affa65691005510c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embedding MIDI Files",
   "id": "7cf89f36ab5f488f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method uses the work done by Ching-Hua Chuan and Dorien Herremans:\n",
    "\n",
    "They proposed the idea of encoding polyphonic music tracks geometrically, allowing a deep learning algorithm to capture the nuances of both polyphony and time in a musical arrangement. They proposed using the Tonnetz, a lattice diagram that relates tones in two-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "I decided to make use of Torchvision's library and modules for encoding imagery for deep learning tasks. I also used the pretty_midi and music21 libraries to assist in deciphering MIDI files.\n",
    "\n",
    "Finally, the encoding process will make use of a Convolutional Neural Network for auto-encoding, and an LSTM for sequence prediction.\n",
    "\n",
    "---\n",
    "\n",
    "Features:\n",
    "1. Converts MIDI files into Tonnetz images\n",
    "2. Trains a CNN Autoencoder to learn tonal/chord features\n",
    "3. Trains an LSTM sequence predictor to predict the next frame in the sequence"
   ],
   "id": "2a9f879001aac8b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Requirements\n",
    "\n",
    "```\n",
    "pip install torch torchvision pretty_midi numpy music21 tqdm\n",
    "```"
   ],
   "id": "fd058d36430b7088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN & LSTM Model Creation",
   "id": "7291d4381edfafab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loader: Custom Dataset Class",
   "id": "24e22eb5693b9edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) parse MIDIs to tonnetz sequences\n",
    "#    -- uses pre-partitioned subfolders\n",
    "partitions = [\"train\", \"valid\", \"test\"]\n",
    "partition_sequences = {}\n",
    "\n",
    "print(\"\\n=== DEBUG: STARTING DATA LOAD ===\\n\")\n",
    "print(f\"Base midi_folder = {DATA_PATH}\")\n",
    "print(f\"Expected subfolders = {partitions}\\n\")\n",
    "\n",
    "for part in partitions:\n",
    "\n",
    "    folder = os.path.join(DATA_PATH, part)\n",
    "    print(f\"\\n--- Checking partition: {part} ---\")\n",
    "    print(f\"Looking for folder: {folder}\")\n",
    "\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"WARNING: Folder does NOT exist: {folder}\")\n",
    "        partition_sequences[part] = []\n",
    "        continue\n",
    "\n",
    "    # list files\n",
    "    midi_paths = (\n",
    "        glob.glob(os.path.join(folder, \"*.mid\")) +\n",
    "        glob.glob(os.path.join(folder, \"*.midi\"))\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(midi_paths)} MIDI files in {folder}\")\n",
    "\n",
    "    if len(midi_paths) == 0:\n",
    "        print(\"WARNING: Folder contains NO MIDI FILES.\")\n",
    "        partition_sequences[part] = []\n",
    "        continue\n",
    "\n",
    "    sequences = []\n",
    "    print(f\"Converting MIDI -> tonnetz sequences [{part}]\")\n",
    "\n",
    "    for idx, p in enumerate(midi_paths):\n",
    "        print(f\"  [{idx+1}/{len(midi_paths)}] Processing: {p}\")\n",
    "        try:\n",
    "            seq = midi_to_tonnetz_sequence(\n",
    "                p,\n",
    "                rows=24,\n",
    "                cols=12,\n",
    "                quantize_beat=1.0\n",
    "            )\n",
    "            print(f\"      ✓ Loaded. Shape = {seq.shape}\")\n",
    "\n",
    "            if seq.shape[0] > 8:\n",
    "                sequences.append(seq)\n",
    "                print(\"      ✓ Added to dataset.\")\n",
    "            else:\n",
    "                print(f\"\\033[91m      ✗ Skipped: sequence too short (< 8 frames).\\033[0m\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\033[91m      ✗ ERROR processing file:\\033[0m\")\n",
    "            print(f\"\\033[91m        {e}\\033[0m\")\n",
    "\n",
    "    partition_sequences[part] = sequences\n",
    "    print(f\"Finished partition '{part}'. Sequences loaded: {len(sequences)}\")\n",
    "\n",
    "# Assign explicitly to train/valid/test variables\n",
    "train_seqs = partition_sequences[\"train\"]\n",
    "val_seqs   = partition_sequences[\"valid\"]\n",
    "test_seqs  = partition_sequences[\"test\"]\n",
    "\n",
    "print(\"\\n=== FINAL COUNTS ===\")\n",
    "print(f\"Train sequences: {len(train_seqs)}\")\n",
    "print(f\"Valid sequences: {len(val_seqs)}\")\n",
    "print(f\"Test sequences:  {len(test_seqs)}\")\n",
    "print(\"======================\\n\")"
   ],
   "id": "2eeb6edb2075e17e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "ca2442840c5d162a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Autoencoder",
   "id": "a49f9189f3fe0845"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) create dataset and dataloader\n",
    "seq_len = 16\n",
    "train_ds = TonnetzSequenceDataset(train_seqs, seq_len=seq_len)\n",
    "val_ds = TonnetzSequenceDataset(val_seqs, seq_len=seq_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "#-- Counting occurrences of 0 and 1 across the entire train_loader\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for X, _ in train_loader:\n",
    "    # Flatten the batch and count\n",
    "    flat = X.flatten()\n",
    "    count_0 += (flat == 0).sum().item()\n",
    "    count_1 += (flat == 1).sum().item()\n",
    "\n",
    "print(\"Total count of 0 in train_loader:\", count_0)\n",
    "print(\"Total count of 1 in train_loader:\", count_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3) initialize autoencoder model\n",
    "autoenc = ConvAutoencoder(in_channels=1, feat_maps=(20,10), rows=24, cols=12, latent_dim=128)"
   ],
   "id": "84f7d536e7fad9c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4) pre-train autoencoder using all frames from training sequences\n",
    "print(\"Pretraining autoencoder\")\n",
    "autoenc = pretrain_autoencoder(autoenc, train_loader, device=device, criterion=\"l1\", epochs=8, lr=1e-2, verbose=True, debug=False)"
   ],
   "id": "3750a27327ad0d02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Saving autoencoder model\n",
    "torch.save(autoenc.state_dict(), f\"{PROJECT_PATH}/models/autoencoder_final.pt\")"
   ],
   "id": "f0dfc5c93df0b8b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sequence Predictor",
   "id": "2cc12be4eacffd28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5) initialize sequence model\n",
    "seq_model = SequencePredictor(latent_dim=128, hidden_dim=256, num_layers=2, out_size=(1,24,12))"
   ],
   "id": "a23e762c653204c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6) train sequence model (freeze encoder)\n",
    "print(\"Training sequence model\")\n",
    "seq_model, train_hist, valid_hist = train_sequence_model(\n",
    "    autoenc, seq_model,\n",
    "    train_loader, val_loader,\n",
    "    device=device,\n",
    "    epochs=25, lr=2e-4,\n",
    "    freeze_encoder=True,\n",
    "    verbose=True,\n",
    "    patience=5, min_delta=1e-4\n",
    ")\n",
    "\n",
    "df_history = pd.DataFrame({\n",
    "    \"train_loss\": train_hist,\n",
    "    \"val_loss\": valid_hist\n",
    "})"
   ],
   "id": "4b334881f3c06e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Saving sequence model\n",
    "\n",
    "torch.save(seq_model.state_dict(), f\"{PROJECT_PATH}/models/sequence_model_final.pt\")"
   ],
   "id": "d86163d477bf9a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plotting History",
   "id": "c325ee19a65a03d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_training_history(train_hist, val_hist, title=\"Training History\"):\n",
    "    epochs = np.arange(1, len(train_hist) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_hist, label=\"Training Loss\", color=\"green\", marker=\"o\")\n",
    "    plt.plot(epochs, val_hist, label=\"Validation Loss\", color=\"deeppink\", marker=\"o\")\n",
    "\n",
    "    plt.xticks(np.arange(0, len(train_hist) + 1, 1))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "ced80ebf2aadf80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_hist = df_history['train_loss'].to_list()\n",
    "val_hist = df_history['val_loss'].to_list()\n",
    "\n",
    "plot_training_history(train_hist, val_hist, title=\"Training History - Music Prediction LSTM\")"
   ],
   "id": "934d202b28639717",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
