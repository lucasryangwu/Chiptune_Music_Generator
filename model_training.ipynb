{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM Model Training",
   "id": "c76912b91d700c57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "d4d699c7db72b3d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pygame')\n",
    "\n",
    "from funcs import *"
   ],
   "id": "c0f41cec685468af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining Constants",
   "id": "4d4ea74d49e623d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# File Paths\n",
    "PROJECT_PATH = PROJECT_PATH = Path.cwd()  # Assumes notebook is run from project root\n",
    "DATA_PATH = f\"{PROJECT_PATH}music/cleaned_data/\"    # must have 'train', 'test', and 'valid' subfolders\n",
    "\n",
    "# Other variables\n",
    "CIRCLE_OF_FIFTHS = [0, 7, 2, 9, 4, 11, 6, 1, 8, 3, 10, 5]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # (testing with cpu)"
   ],
   "id": "affa65691005510c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embedding MIDI Files",
   "id": "7cf89f36ab5f488f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method uses the work done by Ching-Hua Chuan and Dorien Herremans:\n",
    "\n",
    "They proposed the idea of encoding polyphonic music tracks geometrically, allowing a deep learning algorithm to capture the nuances of both polyphony and time in a musical arrangement. They proposed using the Tonnetz, a lattice diagram that relates tones in two-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "I decided to make use of Torchvision's library and modules for encoding imagery for deep learning tasks. I also used the pretty_midi and music21 libraries to assist in deciphering MIDI files.\n",
    "\n",
    "Finally, the encoding process will make use of a Convolutional Neural Network for auto-encoding, and an LSTM for sequence prediction.\n",
    "\n",
    "---\n",
    "\n",
    "Features:\n",
    "1. Converts MIDI files into Tonnetz images\n",
    "2. Trains a CNN Autoencoder to learn tonal/chord features\n",
    "3. Trains an LSTM sequence predictor to predict the next frame in the sequence"
   ],
   "id": "2a9f879001aac8b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Requirements\n",
    "\n",
    "```\n",
    "pip install torch torchvision pretty_midi numpy music21 tqdm\n",
    "```"
   ],
   "id": "fd058d36430b7088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN & LSTM Model Creation",
   "id": "7291d4381edfafab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loader: Custom Dataset Class",
   "id": "24e22eb5693b9edd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TonnetzSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Precomputed: list of tonnetz sequences (numpy arrays of shape (T, R, C))\n",
    "    We will extract sliding windows: given sequence length seq_len, we produce:\n",
    "      X: (seq_len, R, C), y: (R, C) the next slice after the seq\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: List[np.ndarray], seq_len: int = 16):\n",
    "        self.samples = []\n",
    "        self.seq_len = seq_len\n",
    "        for seq in sequences:\n",
    "            T = seq.shape[0]\n",
    "            if T <= seq_len:\n",
    "                continue\n",
    "            # sliding windows\n",
    "            for i in range(0, T - seq_len):\n",
    "                X = seq[i:i+seq_len]      # (seq_len, R, C)\n",
    "                y = seq[i+seq_len]        # (R, C)\n",
    "                self.samples.append((X, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.samples[idx]\n",
    "        # return tensors: (seq_len, channels=1, H, W) and (1, H, W)\n",
    "        Xt = torch.tensor(X).unsqueeze(1)  # 1 channel\n",
    "        yt = torch.tensor(y).unsqueeze(0)\n",
    "        return Xt, yt"
   ],
   "id": "d1a812e923b36968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1) parse MIDIs to tonnetz sequences\n",
    "#    -- uses pre-partitioned subfolders\n",
    "partitions = [\"train\", \"valid\", \"test\"]\n",
    "partition_sequences = {}\n",
    "\n",
    "print(\"\\n=== DEBUG: STARTING DATA LOAD ===\\n\")\n",
    "print(f\"Base midi_folder = {DATA_PATH}\")\n",
    "print(f\"Expected subfolders = {partitions}\\n\")\n",
    "\n",
    "for part in partitions:\n",
    "\n",
    "    folder = os.path.join(DATA_PATH, part)\n",
    "    print(f\"\\n--- Checking partition: {part} ---\")\n",
    "    print(f\"Looking for folder: {folder}\")\n",
    "\n",
    "    if not os.path.isdir(folder):\n",
    "        print(f\"WARNING: Folder does NOT exist: {folder}\")\n",
    "        partition_sequences[part] = []\n",
    "        continue\n",
    "\n",
    "    # list files\n",
    "    midi_paths = (\n",
    "        glob.glob(os.path.join(folder, \"*.mid\")) +\n",
    "        glob.glob(os.path.join(folder, \"*.midi\"))\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(midi_paths)} MIDI files in {folder}\")\n",
    "\n",
    "    if len(midi_paths) == 0:\n",
    "        print(\"WARNING: Folder contains NO MIDI FILES.\")\n",
    "        partition_sequences[part] = []\n",
    "        continue\n",
    "\n",
    "    sequences = []\n",
    "    print(f\"Converting MIDI -> tonnetz sequences [{part}]\")\n",
    "\n",
    "    for idx, p in enumerate(midi_paths):\n",
    "        print(f\"  [{idx+1}/{len(midi_paths)}] Processing: {p}\")\n",
    "        try:\n",
    "            seq = midi_to_tonnetz_sequence(\n",
    "                p,\n",
    "                rows=24,\n",
    "                cols=12,\n",
    "                quantize_beat=1.0\n",
    "            )\n",
    "            print(f\"      ✓ Loaded. Shape = {seq.shape}\")\n",
    "\n",
    "            if seq.shape[0] > 8:\n",
    "                sequences.append(seq)\n",
    "                print(\"      ✓ Added to dataset.\")\n",
    "            else:\n",
    "                print(f\"\\033[91m      ✗ Skipped: sequence too short (< 8 frames).\\033[0m\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\033[91m      ✗ ERROR processing file:\\033[0m\")\n",
    "            print(f\"\\033[91m        {e}\\033[0m\")\n",
    "\n",
    "    partition_sequences[part] = sequences\n",
    "    print(f\"Finished partition '{part}'. Sequences loaded: {len(sequences)}\")\n",
    "\n",
    "# Assign explicitly to train/valid/test variables\n",
    "train_seqs = partition_sequences[\"train\"]\n",
    "val_seqs   = partition_sequences[\"valid\"]\n",
    "test_seqs  = partition_sequences[\"test\"]\n",
    "\n",
    "print(\"\\n=== FINAL COUNTS ===\")\n",
    "print(f\"Train sequences: {len(train_seqs)}\")\n",
    "print(f\"Valid sequences: {len(val_seqs)}\")\n",
    "print(f\"Test sequences:  {len(test_seqs)}\")\n",
    "print(\"======================\\n\")"
   ],
   "id": "2eeb6edb2075e17e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Models",
   "id": "600ad40a12edab03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Autoencoder (CNN)",
   "id": "d3b85f0e3290fa2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    \"\"\"2-layer convolutional encoder + FC decoder to reconstruct tonnetz image\"\"\"\n",
    "    def __init__(self, in_channels=1, feat_maps=(20,10), rows=24, cols=12, latent_dim=128):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(in_channels, feat_maps[0], kernel_size=3, padding=1)  # preserve shape\n",
    "        self.enc_pool1 = nn.MaxPool2d((2,2))   # reduce\n",
    "        self.enc_conv2 = nn.Conv2d(feat_maps[0], feat_maps[1], kernel_size=3, padding=1)\n",
    "        # second pooling (as paper: 2x1)\n",
    "        self.enc_pool2 = nn.MaxPool2d((2,1))\n",
    "        # compute shape after conv/pool with given rows, cols\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, rows, cols)\n",
    "            x = self.enc_conv1(dummy); x = self.enc_pool1(x)\n",
    "            x = self.enc_conv2(x); x = self.enc_pool2(x)\n",
    "            self.enc_out_shape = x.shape  # (1, C, H, W)\n",
    "            enc_flat = int(np.prod(self.enc_out_shape[1:]))\n",
    "        self.fc_enc = nn.Linear(enc_flat, latent_dim)\n",
    "        # Decoder: mirror\n",
    "        self.fc_dec = nn.Linear(latent_dim, enc_flat)\n",
    "        self.dec_convT1 = nn.ConvTranspose2d(feat_maps[1], feat_maps[0], kernel_size=3, padding=1)\n",
    "        self.unpool1 = nn.Upsample(scale_factor=(2,1), mode='nearest')\n",
    "        self.dec_convT2 = nn.ConvTranspose2d(feat_maps[0], in_channels, kernel_size=3, padding=1)\n",
    "        self.unpool2 = nn.Upsample(scale_factor=(2,2), mode='nearest')\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.activation(self.enc_conv1(x))\n",
    "        x = self.enc_pool1(x)\n",
    "        x = self.activation(self.enc_conv2(x))\n",
    "        x = self.enc_pool2(x)\n",
    "        batch = x.shape[0]\n",
    "        flat = x.view(batch, -1)\n",
    "        z = self.fc_enc(flat)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        batch = z.shape[0]\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(batch, *tuple(self.enc_out_shape[1:]))  # (B, C, H, W)\n",
    "        x = self.activation(self.dec_convT1(x))\n",
    "        x = self.unpool1(x)\n",
    "        x = self.activation(self.dec_convT2(x))\n",
    "        x = self.unpool2(x)\n",
    "        # final reconstruction logits (no sigmoid here)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        recon_logits = self.decode(z)\n",
    "        return recon_logits, z\n"
   ],
   "id": "d83ff925e6304d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sequence Predictor (LSTM)",
   "id": "cc016fa5dcf4f25b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SequencePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM sequence model that takes latent vectors sequence and predicts next frame (tonnetz)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, hidden_dim=256, num_layers=2, out_size=(1,24,12)):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=latent_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, np.prod(out_size))  # predict entire tonnetz image flattened\n",
    "        self.out_size = out_size\n",
    "    def forward(self, z_seq):\n",
    "        # z_seq: (B, seq_len, latent_dim)\n",
    "        output, (h_n, c_n) = self.lstm(z_seq)  # output (B, seq_len, hidden_dim)\n",
    "        last = output[:, -1, :]  # take last output\n",
    "        logits = self.fc(last)\n",
    "        logits = logits.view(-1, *self.out_size)  # (B, 1, H, W)\n",
    "        return logits\n"
   ],
   "id": "9c4dfb72c80e169f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training Helper Functions",
   "id": "7426f1ab714f861c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pretrain_autoencoder(autoenc: ConvAutoencoder, dataloader, device,\n",
    "                         criterion = \"logit\", epochs=10, lr=1e-3,\n",
    "                         verbose=False, debug=False):\n",
    "    autoenc.to(device)\n",
    "    optim = torch.optim.Adam(autoenc.parameters(), lr=lr)\n",
    "    if criterion == \"logit\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif criterion == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif criterion == \"l1\":\n",
    "        criterion = nn.L1Loss()\n",
    "    autoenc.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        # Determine the interval: every 1% of total batches\n",
    "        if verbose and num_batches > 0:\n",
    "            interval = max(1, num_batches // 100)\n",
    "            print(f\"\\n=== Autoencoder Pretraining Epoch {epoch+1}/{epochs} ===\")\n",
    "            print(f\"Total batches = {num_batches} | Printing every {interval} batches (~1%)\")\n",
    "\n",
    "        for batch_idx, (X, _) in enumerate(dataloader):\n",
    "            B, seq_len, ch, H, W = X.shape\n",
    "            images = X.view(B * seq_len, ch, H, W).to(device)\n",
    "\n",
    "            if debug:\n",
    "                with torch.no_grad():\n",
    "                    recon, _ = autoenc(images)\n",
    "                    print(\"RECON min/max:\", recon.min().item(), recon.max().item())\n",
    "\n",
    "\n",
    "            optim.zero_grad()\n",
    "            recon_logits, _ = autoenc(images)\n",
    "            loss = criterion(recon_logits, images)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            batch_loss = loss.item() * images.size(0)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            # Print progress every 1% of total batches\n",
    "            if verbose and (batch_idx % interval == 0):\n",
    "                pct = int((batch_idx / num_batches) * 100)\n",
    "                print(f\"  Training: {pct}% Complete. \"\n",
    "                      f\"(Batch {batch_idx+1}/{num_batches}: loss={loss.item():.6f})\")\n",
    "\n",
    "        # Compute average loss\n",
    "        avg = (total_loss /\n",
    "               (len(dataloader.dataset) * dataloader.batch_size)\n",
    "               if len(dataloader) > 0 else total_loss)\n",
    "\n",
    "        print(f\"[AE] Epoch {epoch+1}/{epochs} avg_loss={avg:.6f}\")\n",
    "\n",
    "    return autoenc"
   ],
   "id": "3bdc13693960aa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_sequence_model(\n",
    "        autoenc: ConvAutoencoder,\n",
    "        seq_model: SequencePredictor,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        epochs=30,\n",
    "        lr=1e-3,\n",
    "        freeze_encoder=True,\n",
    "        verbose=False,\n",
    "        patience=5,\n",
    "        min_delta=1e-4):\n",
    "\n",
    "    history_train = []\n",
    "    history_val   = []\n",
    "\n",
    "    # freeze encoder if requested\n",
    "    if freeze_encoder:\n",
    "        for p in autoenc.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    autoenc.to(device)\n",
    "    seq_model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, seq_model.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # ---------------------------\n",
    "    # MAIN TRAINING LOOP\n",
    "    # ---------------------------\n",
    "    for epoch in range(epochs):\n",
    "        seq_model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        # progress interval (~1%)\n",
    "        interval = max(1, num_batches // 100) if verbose else None\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Sequence Model Epoch {epoch+1}/{epochs} ===\")\n",
    "            print(f\"Total training batches = {num_batches}\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # TRAINING EPOCH\n",
    "        # ---------------------------\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            B, seq_len, ch, H, W = X.shape\n",
    "\n",
    "            # flatten time frames for encoder\n",
    "            X_flat = X.view(B * seq_len, ch, H, W).to(device)\n",
    "\n",
    "            # extract latents\n",
    "            with torch.no_grad():\n",
    "                _, z_flat = autoenc(X_flat)\n",
    "\n",
    "            # restore time structure: (B, seq_len, latent_dim)\n",
    "            z_seq = z_flat.view(B, seq_len, -1).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            logits = seq_model(z_seq)        # output: (B, 1, H, W)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.item() * B\n",
    "\n",
    "            # 1% progress print\n",
    "            if verbose and (batch_idx % interval == 0):\n",
    "                pct = int((batch_idx / num_batches) * 100)\n",
    "                print(f\"  Training: {pct}% Complete. \"\n",
    "                      f\"(Batch {batch_idx+1}/{num_batches}: loss={loss.item():.6f})\")\n",
    "\n",
    "        train_avg = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # ---------------------------\n",
    "        # VALIDATION EPOCH\n",
    "        # ---------------------------\n",
    "        seq_model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                B, seq_len, ch, H, W = X.shape\n",
    "                X_flat = X.view(B * seq_len, ch, H, W).to(device)\n",
    "\n",
    "                # encode\n",
    "                _, z_flat = autoenc(X_flat)\n",
    "                z_seq = z_flat.view(B, seq_len, -1).to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                logits = seq_model(z_seq)\n",
    "                loss = criterion(logits, y)\n",
    "                val_loss += loss.item() * B\n",
    "\n",
    "        val_avg = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        history_train.append(train_avg)\n",
    "        history_val.append(val_avg)\n",
    "\n",
    "        print(f\"[SEQ] Epoch {epoch+1}/{epochs} train={train_avg:.6f} val={val_avg:.6f}\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # EARLY STOPPING CHECK\n",
    "        # ---------------------------\n",
    "        if val_avg + min_delta < best_val_loss:\n",
    "            best_val_loss = val_avg\n",
    "            best_state = seq_model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "            if verbose:\n",
    "                print(f\"  ✓ New best validation loss: {best_val_loss:.6f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if verbose:\n",
    "                print(f\"  ✗ No improvement ({epochs_without_improvement}/{patience})\")\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping triggered — restoring best sequence model.\")\n",
    "                seq_model.load_state_dict(best_state)\n",
    "                return seq_model\n",
    "\n",
    "    # restore best model at end\n",
    "    if best_state is not None:\n",
    "        seq_model.load_state_dict(best_state)\n",
    "\n",
    "    return seq_model, history_train, history_val"
   ],
   "id": "e0eca653cf52f5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "ca2442840c5d162a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Autoencoder",
   "id": "a49f9189f3fe0845"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) create dataset and dataloader\n",
    "seq_len = 16\n",
    "train_ds = TonnetzSequenceDataset(train_seqs, seq_len=seq_len)\n",
    "val_ds = TonnetzSequenceDataset(val_seqs, seq_len=seq_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "#-- Counting occurrences of 0 and 1 across the entire train_loader\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for X, _ in train_loader:\n",
    "    # Flatten the batch and count\n",
    "    flat = X.flatten()\n",
    "    count_0 += (flat == 0).sum().item()\n",
    "    count_1 += (flat == 1).sum().item()\n",
    "\n",
    "print(\"Total count of 0 in train_loader:\", count_0)\n",
    "print(\"Total count of 1 in train_loader:\", count_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3) initialize autoencoder model\n",
    "autoenc = ConvAutoencoder(in_channels=1, feat_maps=(20,10), rows=24, cols=12, latent_dim=128)"
   ],
   "id": "84f7d536e7fad9c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4) pre-train autoencoder using all frames from training sequences\n",
    "print(\"Pretraining autoencoder\")\n",
    "autoenc = pretrain_autoencoder(autoenc, train_loader, device=device, criterion=\"l1\", epochs=8, lr=1e-2, verbose=True, debug=False)"
   ],
   "id": "3750a27327ad0d02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Saving autoencoder model\n",
    "torch.save(autoenc.state_dict(), f\"{PROJECT_PATH}/models/autoencoder_final.pt\")"
   ],
   "id": "f0dfc5c93df0b8b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sequence Predictor",
   "id": "2cc12be4eacffd28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5) initialize sequence model\n",
    "seq_model = SequencePredictor(latent_dim=128, hidden_dim=256, num_layers=2, out_size=(1,24,12))"
   ],
   "id": "a23e762c653204c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6) train sequence model (freeze encoder)\n",
    "print(\"Training sequence model\")\n",
    "seq_model, train_hist, valid_hist = train_sequence_model(\n",
    "    autoenc, seq_model,\n",
    "    train_loader, val_loader,\n",
    "    device=device,\n",
    "    epochs=25, lr=2e-4,\n",
    "    freeze_encoder=True,\n",
    "    verbose=True,\n",
    "    patience=5, min_delta=1e-4\n",
    ")\n",
    "\n",
    "df_history = pd.DataFrame({\n",
    "    \"train_loss\": train_hist,\n",
    "    \"val_loss\": valid_hist\n",
    "})"
   ],
   "id": "4b334881f3c06e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Saving sequence model\n",
    "\n",
    "torch.save(seq_model.state_dict(), f\"{PROJECT_PATH}/models/sequence_model_final.pt\")"
   ],
   "id": "d86163d477bf9a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plotting History",
   "id": "c325ee19a65a03d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_training_history(train_hist, val_hist, title=\"Training History\"):\n",
    "    epochs = np.arange(1, len(train_hist) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_hist, label=\"Training Loss\", color=\"green\", marker=\"o\")\n",
    "    plt.plot(epochs, val_hist, label=\"Validation Loss\", color=\"deeppink\", marker=\"o\")\n",
    "\n",
    "    plt.xticks(np.arange(0, len(train_hist) + 1, 1))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "ced80ebf2aadf80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_hist = df_history['train_loss'].to_list()\n",
    "val_hist = df_history['val_loss'].to_list()\n",
    "\n",
    "plot_training_history(train_hist, val_hist, title=\"Training History - Music Prediction LSTM\")"
   ],
   "id": "934d202b28639717",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
